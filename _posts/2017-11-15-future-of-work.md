---
layout: post
title: "Automation and Work, Part I: Will Robots Take All Of Our Jobs?"
category: "Economics"
date: 2017-11-15
---

*This post is part of a three-part series on the impact of automation and artificial intelligence on the labor market, the economy and its consequences for society. These posts are a way for me to organize my thoughts on the topic and maybe find and raise some issues or questions which are currently missing from the discourse.*

<br/>

Over the last few months, newspapers and magazines such as the [New York Times](https://www.nytimes.com/2017/03/28/upshot/evidence-that-robots-are-winning-the-race-for-american-jobs.html), [Wired](https://www.wired.com/story/americans-love-automation-when-it-costs-someone-else-a-job/), [Der Spiegel](http://www.spiegel.de/wirtschaft/zukunft-der-arbeit-kollege-computer-und-die-angst-vor-dem-maschinenwinter-a-1163946.html) or [Die Zeit](http://www.zeit.de/arbeit/2017-09/kuenstliche-intelligenz-roboter-arbeitsmarkt-studie) as well as other publications have published countless articles on robots replacing human workers, mainly in the manufacturing industry, in the not-too-distant future. In my own work on industrial robotics, I notice that in many sectors, repetitive or physically straining work can already easily replaced by off-the-shelf robotic solutions; other, less mechanical jobs, such as those requiring a sense of "touch" or dealing with flexible components, will be replaceable by the kinds of machines me and my colleagues are developing today.<!--more--> Even on highly complex and high-risk tasks such as surgery, [robotic systems are outperforming humans at some tasks](https://spectrum.ieee.org/the-human-os/biomedical/devices/in-fleshcutting-task-autonomous-robot-surgeon-beats-human-surgeons). However, many of the articles published are based on little factual evidence and are influenced by the recent wave of frequent, but isolated high-profile news about breakthroughs in the field of artificial intelligence. Others confound automation with the parallel and related but much more general trend toward digitalization. So, in order to assess the potential impacts of digitalization, artificial intelligence and automation on the job market and on society as a whole, it is necessary to properly discriminate between the concepts and assess the current state of the art and prospects in each of them before assessing the validity of some of the claims made about robots taking all the jobs.

## Digitalization, Artificial Intelligence and Automation: Where Do We Stand?

[Digitalization](https://de.wikipedia.org/wiki/Digitalisierung) is the replacement of analog processes by digital equivalents. As per this definition, this transformation is largely completed: 10 years ago, in 2007, [94% of all information in the world has been stored digitally](http://science.sciencemag.org/content/332/6025/60). Indeed, some argue that digitalization has been driving the [Third Industrial Revolution](https://de.wikipedia.org/wiki/Digitale_Revolution), shifting employment (and human productivity) away from manufacturing and towards the service and financial sectors.[^1] So digitalization is nothing new - it has had a profound impact on all sectors of the economy for at least thirty years, but did not cause massive job losses: Humans began working alongside the new machines, using them to boost their productivity.

Artificial intelligence has not been completely defined, but can be described as the ["science and engineering of making intelligent machines"](http://www-formal.stanford.edu/jmc/whatisai/node1.html). While this definition is not very specific, research in the domain of artificial intelligence now touches on a vast array of subjects, ranging from [robotics](https://research.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html) to [image processing](https://groups.csail.mit.edu/graphics/hdrnet/) or [autonomous cars](https://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-driveai-is-mastering-autonomous-driving-with-deep-learning). As opposed to digitalization, which has been transforming the economy and the labor market for more than three decades and the impact of which is well understood, economically viable applications for artificial intelligence have emerged only over the past few years, but in an astonishingly wide array of domains (such as [retail](https://www.forbes.com/sites/bernardmarr/2017/08/29/how-walmart-is-using-machine-learning-ai-iot-and-big-data-to-boost-retail-performance/#6e1330436cb1), [health care](https://academic.oup.com/bib/article-abstract/doi/10.1093/bib/bbx044/3800524?redirectedFrom=fulltext), [politics](http://www.independent.co.uk/news/long_reads/artificial-intelligence-democracy-elections-trump-brexit-clinton-a7883911.html) or [law enforcement](https://www.forbes.com/sites/bernardmarr/2017/09/19/how-robots-iot-and-artificial-intelligence-are-transforming-the-police/#31eb0fa35d61)). However, the range of applications as well as the astonishing feats artificial intelligence systems achieved on [very specific tasks](https://www.technologyreview.com/s/609141/alphago-zero-shows-machines-can-become-superhuman-without-any-help/), receiving probably disproportionate media coverage, are somewhat deceiving. While artificial intelligence absolutely has the potential to solve many problems which cannot yet be efficiently solved, it remains [highly dependent](https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/) both on the *amount* and *quality* of the training data it receives. There are [efforts](https://www.wired.com/story/googles-ai-wizard-unveils-a-new-twist-on-neural-networks/) to develop artificial neural networks which learn more like humans do (requiring much less training data) and the amount of available data continues to grow, in the manufacturing industry as well as in other sectors, but the notion many people have about artificial intelligence - "just slap AI onto a problem and it's just going to work" - is misguided. The (still very large) amounts of input data do, at least in the manufacturing industry, not simply appear out of thin air, nicely formatted and stored in a database our artificial intelligence can talk to. Neural networks cannot ([in most cases](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.1268&rep=rep1&type=pdf)) automatically infer relevant features and preprocess the input data to its liking. Neither can they reliably guarantee perfect classifications or predictions - which can have terrible consequences in safety-critical applications. In practice, modern factories have highly heterogeneous IT infrastructures, with legacy devices, compatibility layers and very costly equipment that cannot easily be replaced, and there is not yet one single standard for industrial information interchange to rule them all (although [there is a promising contender](https://opcfoundation.org/about/opc-technologies/opc-ua/)). There is also a widespread reluctance among industry professionals to have their expensive and often dangerous equipment obey the ultimately non-deterministic whims of an artificial intelligence, so hybrid systems must be developed in which "traditional" automation technology is augmented and assisted by artificial intelligence while retaining full control over the safety-critical systems. So, legions of data scientists, automation engineers and computer scientists will have to work on bringing artificial intelligence to the manufacturing industry - it will not happen overnight, and it will require a lot of human research and engineering in the process.

In the whole debate about jobs being lost and the economy being transformed, "automation" is often used somewhat like "digitalization", even though it is (at least to me) a much more interesting case. In the manufacturing industry, "traditional" automation means having a machine (a CNC machine, an industrial robot or a custom-built assembly station) perform some task, such as welding two parts of a car chassis together. They are programmed exactly once, and the program is executed repeatedly, does not change between executions, and the machine keeps executing precisely the same task over and over again. Most routine manual tasks which can easily be automated have already been automated during the second and third industrials revolutions.[^1] Some tasks are currently being automated, notably those requiring a "sense of touch" - force-torque sensors and force-controlled robotic arms [have been around for a while](http://ieeexplore.ieee.org/document/262527/), but have only recently found widespread adaption in the manufacturing industry. However, force control and similar methods merely expand the range of automatable tasks at around the same rate it is currently happening. Only a fusion of artificial intelligence and advanced robotics would have a truly transformational impact on the manufacturing industry (and many other economic sectors such as health care) - only when robotic systems begin to *semantically understand* the task they are supposed to perform and the environment in which they operate can they replace human workers, both blue- and white-collar, on the scale many media publications are forecasting. However, at least presently, this has not been realized. Artificial intelligence systems today only excel at the specific, isolated tasks they were trained to excel at. There are many attempts at making [robots](http://neurosciencenews.com/robotics-learning-neurodevelopment-3187/) [learn](https://www.technologyreview.com/s/601045/this-factory-robot-learns-a-new-job-overnight/), many of which are promising. But when the technology is ready, it will not instantly replace human workers. Human workers will work alongside robotic workers, and hordes of engineers will be required to deploy and integrate these highly complex systems into existing assembly lines with their own highly context-dependent physical and technological infrastructures and ensure that the resulting systems meet the required safety requirements. Intelligent robots will play an increasingly important role in the economy - but the transformation will not be instantaneous, it will not happen overnight, and it will happen not from digitalization (which is not new), nor from automation (neither of which is new), but from the fusion of artificial intelligence and robotics.

## Which Jobs Will Be Destroyed?

Most mainstream media publications cite [Frey & Osborne's 2013 paper](https://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf) when discussing the potential impact of intelligent robots on the economy. According to the authors, 47% of American jobs are at risk of being replaced by robots. The paper is a very interesting read and their methodology for quantifying the impact of emerging technologies on the labor market is convincing. However, most publications only cite the abstract. There are a few relevant details in the paper which are often not discussed, and a few issues with the paper calling the number of 47% into question.

First, not all jobs are equally affected. Broadly speaking, every job done by a human can be located on two spectra: The manual-cognitive spectrum, which indicates to what extent tasks pertaining to the job involve physical manipulation (an example for a purely manual job is assembling the chassis of an automobile; an example of a purely cognitive job is that of an accountant or a lawyer), and the routine-nonroutine spectrum indicating the difficulty or repetitiveness of the task. Welding two pieces of a car door together is a routine manual job; building a violin, carving a statue or caring for elderly citizens are nonroutine manual jobs. There are also many routine cognitive jobs, such as data entry or early-stage customer service, as opposed to nonroutine cognitive jobs such as journalism, computer programming or furniture design. This immediately leads to the idea that not all jobs are equally threatened - one would assume that routine manual jobs are in greatest danger, as many of them can already be performed with today's industrial robots. At the same time, advanced robotic systems are capable of performing [nonroutine manual jobs](https://www.youtube.com/watch?v=CZjrL1QyULw) and artificial intelligence systems begin to outperform humans at routine cognitive jobs such as [image classification](https://www.eetimes.com/document.asp?doc_id=1325712). Frey & Osborne confirm this: Routine manual jobs have been subject to computerization for several decades and employment in these jobs has consequently declined. They argue that over the next decades, similar developments will unfold for nonrutine manual and routine cognitive jobs. However, the authors make a more interesting claim about the *sectors of the economy* most affected by computerization:

![Frey & Osborne: Probability of Computerization by Sector]({{ "/images/future_of_work/frey_osborne_sectors.png" | absolute_url }}){: .center-image }

As expected, "Production" and "Material Moving" are among the most affected sectors. However, so are "Office and Administrative Support", "Sales and Related" and "Service" - and because these sectors employ a much larger share of the working population, the impact on the labor market by computerization in these sectors will be proportionately larger. It thus seems that the largest transformations in the labor market will not happen in the manufacturing industries, but through artificial intelligence and smart service robots in sectors where cognitive tasks dominate. This challenges the narrative of the "replaced factory worker" as the sole, or even primary, "victim" of emerging technologies, as workers from across the manual-cognitive spectra will be affected.

Likewise, not all workers will be affected at the same time. The above graphic illustrates that there will be two waves of automation, with nonroutine cognitive jobs being automatized at some point in the indefinite future. According to this model, there will be several decades during which a large portion (around 50% of the American workforce) are out of work - unless they can take up nonroutine cognitive jobs. This can cause considerable social tension, particularly because lower-income workers are likely to be affected first, and can increase the growing [gap between the rich and the poor](http://fortune.com/2017/08/01/wealth-gap-america/), which could be devastating for a number of reasons. I will spin some scenarios about the consequences of such a development in the next post.

Nonetheless, and despite its methodological thoroughness, the numbers provided by the paper can be questioned in two ways, and are somewhat problematic in another.

First, the paper draws its data from the judgments of experts as opposed to historical data, simply because no such data exists - artificial intelligence has only begun significantly impacting the economy and the labor market within the past decade. It is therefore likely to suffer from the [overconfidence effect](https://en.wikipedia.org/wiki/Overconfidence_effect), a cognitive bias by which experts overestimate the accuracy of their judgments, as well as [Amara's law](https://en.wikipedia.org/wiki/Roy_Amara#Amara.27s_law) - that the effects of a technology tend to be overestimated in the short term and underestimated in the long term.[^2]

Second, the paper implicitly assumes that most if not all workers in sectors/occupations associated with a particular type of labor actually all perform similar tasks. This, however, is not necessarily true - many accountants, for instance, perform repetitive, data-entry-type tasks, while others (still accountants!) perform more nonroutine tasks such as data analysis, payroll optimization or trainee supervision, and most actually perform a number of different tasks over the course of a workday. Indeed, as [Bonin](ftp://ftp.zew.de/pub/zew-docs/gutachten/Kurzexpertise_BMAS_ZEW2015.pdf) argues, if one distinguishes by the actually performed *tasks* rather than by occupation, a mere 9% of American jobs fall into Frey & Osborne's "high-risk" category. This still represents a large share of the labor market, but is considerably less than the original paper's 47%.

Last, Frey & Osborne do not provide a concrete timeline. Instead, they argue that 47% of jobs are likely to be automatized "at some indeterminate point in the future", and that those jobs with a lower likelihood of automation may be automatized at some later, but equally indeterminate, point in the future - they simply argue *that* changes will happen, but make no claim about *when*. And that's OK - however, the reporting media hardly stress how little we know, not only about concrete figures, but also about the timeline. Maybe this uncertainty is a substantial part of the real problem: It is difficult to productively address an issue you know exists, but about which you know very little.

## What Now?

Reading Frey's and Osborne's paper as well as its criticisms, it seems that the paper rather raises questions than provides answers. While it makes a credible and intuitively highly plausible case that large-scale replacements of the human workforce by computer-based systems will occur, it remains unclear which sectors will be affected to which degree and at which point in time. Conversely, it becomes apparent that numbers such as the often-cited 47% of the American workforce at high risk of replacement must be taken with a grain of salt. In research papers, one often concludes that further research is necessary to shed light on specific aspects of a problem. Here, however, this is difficult, because there is no data about the impact of technologies which are currently in development or which have not yet been developed. So all studies addressing this topic must contain a degree of speculation about the future - which is why, when assessing the possible impacts of this transformation on the labor market and society as a whole, it is of crucial importance to not only consider the scenario considered the most *likely*, but also other scenarios which may have completely different effects and may require different solutions.

***

[^1]: The term "Third Industrial Revolution" is ill-defined. In Germany, many economists, computer scientists and automation engineers declare the Third Industrial Revolution as largely completed and superseded by the "Fourth Industrial Revolution", or ["Industry 4.0"](https://www.forbes.com/sites/bernardmarr/2016/06/20/what-everyone-must-know-about-industry-4-0/#1c7c4ff6795f) (an equally ill-defined concept deserving a separate post). In the Anglo-Saxon world, the term includes both the "Digital Revolution" and the subsequent (or current) attempts to establish a ["smart green digital economy"](https://www.huffingtonpost.com/jeremy-rifkin/third-industrial-revolution-green-economy_b_8286142.html), which (partly) overlaps with the continental-European "Industry 4.0".

[^2]: Although Amara's law is often cited, I have not yet found empirical studies testing the hypothesis. Maybe there should be another blog post about it.
